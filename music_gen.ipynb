{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing required libraries**"
      ],
      "metadata": {
        "id": "yZY9pOGBzl3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch matplotlib tqdm livelossplot pretty_midi pydub midi2audio"
      ],
      "metadata": {
        "id": "Ot43F-euzT-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Necessary libraries**"
      ],
      "metadata": {
        "id": "0ScpJgNcum-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "import math\n",
        "import pretty_midi\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from pydub import AudioSegment\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "696ITGyftL1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloading the maestro dataset**\n",
        "\n"
      ],
      "metadata": {
        "id": "GPnvWQUQvHPG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv_f1gSoG7wX"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_zip_path = 'PATH OF DIRECTORY'\n",
        "if not os.path.exists(dataset_zip_path):\n",
        "      !wget https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip -O {dataset_zip_path}\n",
        "      print(f\"Downloaded MAESTRO dataset to {dataset_zip_path}\")\n",
        "else:\n",
        "      print(f\"MAESTRO dataset already exists at {dataset_zip_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extracting the midi files and organizing**"
      ],
      "metadata": {
        "id": "3lYz48A1xBS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define paths\n",
        "dataset_zip_path = 'Path to the downloaded zip file'\n",
        "extracted_subset_path = 'Directory path for extracted filest'\n",
        "\n",
        "# Create the target directory\n",
        "os.makedirs(extracted_subset_path, exist_ok=True)\n",
        "print(f\"Created directory at {extracted_subset_path}\")\n",
        "\n",
        "# Maximum files to extract\n",
        "max_total_files = 700\n",
        "total_extracted = 0\n",
        "\n",
        "# Open the ZIP file\n",
        "with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "\n",
        "    midi_files = [f for f in zip_ref.namelist() if f.endswith(('.midi', '.mid'))]\n",
        "    print(f\"Found {len(midi_files)} MIDI files in the zip archive.\")\n",
        "\n",
        "    # Group files by year\n",
        "    files_by_year = {}\n",
        "    for file in midi_files:\n",
        "        parts = file.split('/')\n",
        "        if len(parts) > 1:  # Ensure there's a subdirectory for the year\n",
        "            year = parts[1]\n",
        "            files_by_year.setdefault(year, []).append(file)\n",
        "\n",
        "    num_years = len(files_by_year)\n",
        "    files_per_year = math.ceil(max_total_files / num_years)\n",
        "\n",
        "    # Extract files from each year\n",
        "    for year, files in files_by_year.items():\n",
        "        extracted_from_year = 0\n",
        "        for file in files:\n",
        "            if extracted_from_year < files_per_year and total_extracted < max_total_files:\n",
        "                # Extract file\n",
        "                zip_ref.extract(file, 'Base directory path')\n",
        "                source_path = os.path.join('Base Directory path', file)\n",
        "                target_path = os.path.join(extracted_subset_path, os.path.basename(file))\n",
        "\n",
        "                # Move the file to the target directory\n",
        "                shutil.move(source_path, target_path)\n",
        "                print(f\"Extracted and moved to: {target_path}\")\n",
        "\n",
        "                extracted_from_year += 1\n",
        "                total_extracted += 1\n",
        "\n",
        "            if total_extracted >= max_total_files:\n",
        "                break\n",
        "\n",
        "        print(f\"Extracted {extracted_from_year} files from year {year}.\")\n",
        "        if total_extracted >= max_total_files:\n",
        "            break\n",
        "\n",
        "print(f\"Extraction complete. Total MIDI files moved: {total_extracted}\")\n"
      ],
      "metadata": {
        "id": "CNmqseLorvgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conversion of MIDI files to Pianoroll data**"
      ],
      "metadata": {
        "id": "0s9vgP-vxpuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Configuration for calm MIDI to Piano Roll Conversion\n",
        "n_tracks = 5\n",
        "n_pitches = 36\n",
        "lowest_pitch = 36\n",
        "measure_resolution = 16\n",
        "n_measures = 4\n",
        "data = []\n",
        "# Directory with the 700 extracted files\n",
        "extracted_subset_path = 'EXTRACTED SUBSET PATH'\n",
        "def midi_to_piano_roll(midi_file):\n",
        "    \"\"\"Convert a MIDI file to a calming piano roll format.\"\"\"\n",
        "    try:\n",
        "        midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
        "        if not midi_data.instruments:\n",
        "           print(f\"No instruments found in file: {midi_file}\")\n",
        "           return None\n",
        "        # Extract the piano roll and limit pitch range\n",
        "        piano_roll = midi_data.instruments[0].get_piano_roll(fs=4 * measure_resolution)\n",
        "        piano_roll = piano_roll[lowest_pitch:lowest_pitch + n_pitches]\n",
        "        piano_roll = (piano_roll > 0).astype(np.float32)\n",
        "        # Trim or pad the piano roll to fit the required length\n",
        "        max_length = n_measures * measure_resolution\n",
        "        if piano_roll.shape[1] > max_length:\n",
        "           piano_roll = piano_roll[:, :max_length]\n",
        "        else:\n",
        "          piano_roll = np.pad(piano_roll, ((0, 0), (0, max_length - piano_roll.shape[1])))\n",
        "        return piano_roll\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {midi_file}: {e}\")\n",
        "        return None\n",
        "# Process and collect data with calm settings\n",
        "success_count = 0\n",
        "failure_count = 0\n",
        "extracted_files = [f for f in os.listdir(extracted_subset_path) if f.endswith(('.midi', '.mid'))]\n",
        "print(f\"Found {len(extracted_files)} MIDI files in the extracted subset directory.\")\n",
        "for midi_file in extracted_files[:700]: # Limit to the first 700 files\n",
        "    midi_path = os.path.join(extracted_subset_path, midi_file)\n",
        "    piano_roll = midi_to_piano_roll(midi_path)\n",
        "    if piano_roll is not None:\n",
        "       data.append(piano_roll)\n",
        "       success_count += 1\n",
        "       print(f\"Successfully converted {success_count} files: {midi_file}\")\n",
        "    else:\n",
        "        failure_count += 1\n",
        "\n",
        "\n",
        "data = np.array(data)\n",
        "print(f\"\\nData shape after conversion: {data.shape}\")\n",
        "print(f\"Total successfully converted files: {success_count}\")\n",
        "print(f\"Total failed conversions: {failure_count}\")"
      ],
      "metadata": {
        "id": "R05Snqb_r-u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Converting data to a PyTorch tensor**"
      ],
      "metadata": {
        "id": "MzAkqM0sx8IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = np.array(data)\n",
        "data = data.reshape(-1, n_tracks, n_measures * measure_resolution, n_pitches)\n",
        "\n",
        "data_tensor = torch.as_tensor(data, dtype=torch.float32)\n",
        "dataset = TensorDataset(data_tensor)\n",
        "data_loader = DataLoader(dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "print(\"Calm DataLoader created.\")\n",
        "print(f\"Data shape for DataLoader: {data_tensor.shape}\")"
      ],
      "metadata": {
        "id": "xII4LonwsFDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generator and Discriminator Functions**"
      ],
      "metadata": {
        "id": "xvxzRfjiyLug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define required global configuration variables\n",
        "n_tracks = 5\n",
        "n_measures = 4\n",
        "measure_resolution = 16\n",
        "n_pitches = 36\n",
        "latent_dim = 128\n",
        "batch_size = 16\n",
        "# Generator Block\n",
        "class GeneraterBlock(nn.Module):\n",
        "      def __init__(self, in_dim, out_dim, kernel, stride):\n",
        "          super().__init__()\n",
        "          self.transconv = nn.ConvTranspose3d(in_dim, out_dim, kernel, stride)\n",
        "          self.batchnorm = nn.BatchNorm3d(out_dim)\n",
        "      def forward(self, x):\n",
        "          x = self.transconv(x)\n",
        "          x = self.batchnorm(x)\n",
        "          return torch.relu(x)\n",
        "# Generator Model\n",
        "class Generator(nn.Module):\n",
        "     def __init__(self):\n",
        "         super().__init__()\n",
        "         self.transconv0 = GeneraterBlock(latent_dim, 256, (4, 1, 1), (4, 1, 1))\n",
        "         self.transconv1 = GeneraterBlock(256, 128, (1, 4, 1), (1, 4, 1))\n",
        "         self.transconv2 = GeneraterBlock(128, 64, (1, 1, 4), (1, 1, 4))\n",
        "         self.transconv3 = GeneraterBlock(64, 32, (1, 1, 3), (1, 1, 1))\n",
        "         self.transconv4 = nn.ModuleList([GeneraterBlock(32, 16, (1, 4, 1), (1, 4, 1)) for _ in range(n_tracks)])\n",
        "         self.transconv5 = nn.ModuleList([GeneraterBlock(16, 1, (1, 1, 12), (1, 1, 12)) for _ in range(n_tracks)])\n",
        "     def forward(self, x):\n",
        "         x = x.view(-1, latent_dim, 1, 1, 1)\n",
        "         x = self.transconv0(x)\n",
        "         x = self.transconv1(x)\n",
        "         x = self.transconv2(x)\n",
        "         x = self.transconv3(x)\n",
        "         x = [transconv(x) for transconv in self.transconv4]\n",
        "         x = torch.cat([transconv(x_) for x_, transconv in zip(x, self.transconv5)], 1)\n",
        "         x = x.view(-1, n_tracks, n_measures * measure_resolution, n_pitches)\n",
        "         return x\n",
        "# Discriminator Block\n",
        "class DiscriminatorBlock(nn.Module):\n",
        "     def __init__(self, in_dim, out_dim, kernel, stride):\n",
        "         super().__init__()\n",
        "         self.conv = nn.Conv3d(in_dim, out_dim, kernel, stride)\n",
        "         self.batchnorm = nn.BatchNorm3d(out_dim)\n",
        "     def forward(self, x):\n",
        "         x = self.conv(x)\n",
        "         x = self.batchnorm(x)\n",
        "         return F.leaky_relu(x)\n",
        "# Updated Discriminator Model with adjusted kernel and stride sizes\n",
        "class Discriminator(nn.Module):\n",
        "      def __init__(self):\n",
        "          super().__init__()\n",
        "          # Adjusted kernel and stride sizes to fit the input shape\n",
        "          self.conv0 = nn.ModuleList([DiscriminatorBlock(1, 16, (1, 1, 3), (1, 1, 1)) for _ in range(n_tracks)])\n",
        "          self.conv1 = nn.ModuleList([DiscriminatorBlock(16, 16, (1, 3, 1), (1, 2, 1)) for _ in range(n_tracks)])\n",
        "          self.conv2 = DiscriminatorBlock(16 * n_tracks, 64, (1, 1, 2), (1, 1, 1))\n",
        "          self.conv3 = DiscriminatorBlock(64, 64, (1, 1, 2), (1, 1, 1))\n",
        "          self.conv4 = DiscriminatorBlock(64, 128, (1, 3, 1), (1, 2, 1))\n",
        "          self.conv5 = DiscriminatorBlock(128, 128, (2, 1, 1), (1, 1, 1))\n",
        "          self.conv6 = DiscriminatorBlock(128, 256, (2, 1, 1), (2, 1, 1))\n",
        "          self.dense = nn.Linear(256, 1)\n",
        "      def forward(self, x):\n",
        "          x = x.view(-1, n_tracks, n_measures, measure_resolution, n_pitches)\n",
        "          x = [conv(x[:, [i]]) for i, conv in enumerate(self.conv0)]\n",
        "          x = torch.cat([conv(x_) for x_, conv in zip(x, self.conv1)], 1)\n",
        "          x = self.conv2(x)\n",
        "          x = self.conv3(x)\n",
        "          x = self.conv4(x)\n",
        "          x = self.conv5(x)\n",
        "          x = self.conv6(x)\n",
        "          x = x.view(-1, 256)\n",
        "          return self.dense(x)"
      ],
      "metadata": {
        "id": "nPaLVDGBsK_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "hVABuvIBybyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize models and move them to GPU\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Optimizers remain the same\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
        "\n",
        "# Gradient penalty function\n",
        "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, 1, device=device)\n",
        "    alpha = alpha.expand_as(real_samples)\n",
        "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
        "    d_interpolates = discriminator(interpolates)\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(d_interpolates, device=device),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty\n",
        "\n",
        "# Training Loop with GPU utilization\n",
        "n_steps = 5\n",
        "for epoch in range(n_steps):\n",
        "    for real_samples in data_loader:\n",
        "        real_samples = real_samples[0].unsqueeze(1).to(device)\n",
        "        latent = torch.randn(batch_size, latent_dim, device=device)\n",
        "\n",
        "        # Multiple discriminator steps\n",
        "        for _ in range(5):\n",
        "            d_optimizer.zero_grad()\n",
        "            prediction_real = discriminator(real_samples)\n",
        "            d_loss_real = -torch.mean(prediction_real)\n",
        "            fake_samples = generator(latent).detach()\n",
        "            prediction_fake_d = discriminator(fake_samples)\n",
        "            d_loss_fake = torch.mean(prediction_fake_d)\n",
        "            gradient_penalty = compute_gradient_penalty(discriminator, real_samples, fake_samples)\n",
        "            d_loss = d_loss_real + d_loss_fake + 10 * gradient_penalty\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "        # Generator step\n",
        "        g_optimizer.zero_grad()\n",
        "        fake_samples = generator(latent)\n",
        "        prediction_fake_g = discriminator(fake_samples)\n",
        "        g_loss = -torch.mean(prediction_fake_g)\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{n_steps}] - D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "aEG2JxnZsQ8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mood configuration of music**"
      ],
      "metadata": {
        "id": "PqkVDG7dyjNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mp3_output_dir = 'OUTPUT DIRECTORY PATH'\n",
        "os.makedirs(mp3_output_dir, exist_ok=True)\n",
        "# Function to configure music based on type/mood\n",
        "def set_mood_config(mood):\n",
        "    config = {}\n",
        "    if mood == 1: # Bittersweet\n",
        "       config[\"programs\"] = [0, 48, 41] # Piano, Strings, and Bass\n",
        "       config[\"track_names\"] = ['Soft Piano', 'Emotional Strings', 'Warm Bass']\n",
        "       config[\"tempo\"] = 50 # Slow, reflective tempo\n",
        "       config[\"lowest_pitch\"] = 36\n",
        "       config[\"allowed_pitches\"] = [36, 39, 43, 46, 50, 53, 57, 60] # Minor intervals for bittersweet emotion\n",
        "       config[\"velocity_range\"] = (35, 50) # Moderate dynamics\n",
        "       config[\"note_duration_factor\"] = 5.5 # Long notes for smooth transitions\n",
        "       config[\"time_step_interval\"] = 10\n",
        "    elif mood == 2: # Uplifting\n",
        "       config[\"programs\"] = [0, 48, 112] # Piano, Strings, and Bells\n",
        "       config[\"track_names\"] = ['Bright Piano', 'Lively Strings', 'Gentle Bells']\n",
        "       config[\"tempo\"] = 120 # Fast tempo for energy and optimism\n",
        "       config[\"lowest_pitch\"] = 40\n",
        "       config[\"allowed_pitches\"] = [40, 43, 47, 50, 54, 57, 60, 64, 67]\n",
        "       config[\"velocity_range\"] = (50, 70) # High dynamics\n",
        "       config[\"note_duration_factor\"] = 4.0 # Shorter overlapping notes\n",
        "       config[\"time_step_interval\"] = 6 # Faster transitions\n",
        "       config[\"min_duration\"] = 20 # Ensure at least 20 seconds of music\n",
        "    elif mood == 3: # Nature-Inspired\n",
        "       config[\"programs\"] = [0, 74, 123] # Piano, Flute, and Birds\n",
        "       config[\"track_names\"] = ['Soft Piano', 'Flute Melody', 'Chirping Birds']\n",
        "       config[\"tempo\"] = 40 # Slow tempo for grounding\n",
        "       config[\"lowest_pitch\"] = 32\n",
        "       config[\"allowed_pitches\"] = [32, 36, 39, 43, 46, 50, 53, 57]\n",
        "       config[\"velocity_range\"] = (25, 40) # Soft dynamics\n",
        "       config[\"note_duration_factor\"] = 7.0 # Long atmospheric notes\n",
        "       config[\"time_step_interval\"] = 12 # Slow rhythm\n",
        "       config[\"chirp_probability\"] = 0.5 # Higher chirp probability\n",
        "       config[\"chirp_overlap_factor\"] = 0.15 # More overlapping chirps\n",
        "\n",
        "    elif mood == 4: # Meditation\n",
        "       config[\"programs\"] = [0, 89, 96] # Piano, Choir, and Ambient Pad\n",
        "       config[\"track_names\"] = ['Calm Piano', 'Soothing Choir', 'Meditative Pad']\n",
        "       config[\"tempo\"] = 30 # Very slow, meditative tempo\n",
        "       config[\"lowest_pitch\"] = 30\n",
        "       config[\"allowed_pitches\"] = [30, 33, 36, 40, 43, 47, 50]\n",
        "       config[\"velocity_range\"] = (20, 35) # Very soft dynamics\n",
        "       config[\"note_duration_factor\"] = 8.0 # Long, sustained notes\n",
        "       config[\"time_step_interval\"] = 15 # Minimal rhythm for tranquility\n",
        "       config[\"volume_increase\"] = 6 # Subtle volume boost for clarity\n",
        "    return config\n",
        "# Function to generate and save a single sample based on the selected configuration\n",
        "def generate_sample(generator, latent_dim, sample_index, config, mood_name):\n",
        "    device = next(generator.parameters()).device\n",
        "    z = torch.randn(1, latent_dim,device=device)\n",
        "    with torch.no_grad():\n",
        "         sample = generator(z).squeeze(0).cpu().numpy()\n",
        "    sample = sample[0]\n",
        "    midi_data = pretty_midi.PrettyMIDI()\n",
        "\n",
        "    for idx, (program, track_name) in enumerate(zip(config[\"programs\"], config[\"track_names\"])):\n",
        "        instrument = pretty_midi.Instrument(program=program, is_drum=False, name=track_name)\n",
        "        for time_step in range(0, sample.shape[1], config[\"time_step_interval\"]):\n",
        "           for pitch in range(sample.shape[2]):\n",
        "               if sample[idx, time_step, pitch] > 0.5 and (pitch + config[\"lowest_pitch\"]) in config[\"allowed_pitches\"]:\n",
        "                  velocity = int(config[\"velocity_range\"][0] + (config[\"velocity_range\"][1] - config[\"velocity_range\"][0]) * torch.rand(1).item())\n",
        "                  note_start = (time_step * (60 / config[\"tempo\"]) / 4)\n",
        "                  note_end = note_start + (60 / config[\"tempo\"]) * config[\"note_duration_factor\"]\n",
        "                  note = pretty_midi.Note(\n",
        "                  velocity=velocity,\n",
        "                  pitch=pitch + config[\"lowest_pitch\"],\n",
        "                  start=note_start,\n",
        "                  end=note_end\n",
        "                  )\n",
        "                  instrument.notes.append(note)\n",
        "           # Add overlapping random chirps for Nature-Inspired\n",
        "           if mood_name == \"Nature-Inspired\" and torch.rand(1).item() < config.get(\"chirp_probability\", 0.5):\n",
        "              for _ in range(torch.randint(1, 3, (1,)).item()): # Generate 1-3 chirps in each interval\n",
        "                  chirp_pitch = config[\"lowest_pitch\"] + torch.randint(0, len(config[\"allowed_pitches\"]), (1,)).item()\n",
        "                  chirp_start = (time_step * (60 / config[\"tempo\"]) / 4) + torch.rand(1).item() * config.get(\"chirp_overlap_factor\", 0.1)\n",
        "                  chirp_note = pretty_midi.Note(\n",
        "                  velocity=int(config[\"velocity_range\"][1]), # Slightly louder for chirps\n",
        "                  pitch=chirp_pitch,\n",
        "                  start=chirp_start,\n",
        "                  end=chirp_start + 0.3 # Short chirp\n",
        "                  )\n",
        "                  instrument.notes.append(chirp_note)\n",
        "        midi_data.instruments.append(instrument)\n",
        "    midi_filename = tempfile.NamedTemporaryFile(suffix=\".mid\", delete=False).name\n",
        "    midi_data.write(midi_filename)\n",
        "    output_wav = f\"{mp3_output_dir}/{mood_name}_sample_{sample_index + 1}.wav\"\n",
        "    output_mp3 = f\"{mp3_output_dir}/{mood_name}_sample_{sample_index + 1}.mp3\"\n",
        "    sound = AudioSegment.from_wav(output_wav)\n",
        "    # Increase volume for Meditation without harsh transitions\n",
        "    if mood_name == \"Meditation\":\n",
        "       sound = sound + config.get(\"volume_increase\", 0)\n",
        "    sound.export(output_mp3, format=\"mp3\")\n",
        "    os.remove(midi_filename)\n",
        "    os.remove(output_wav)\n",
        "    print(f\"Generated {mood_name} MP3 file: {output_mp3}\")"
      ],
      "metadata": {
        "id": "c-1FF5IfsVBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generating music for selected mood**"
      ],
      "metadata": {
        "id": "NRr8-o28y685"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moods = [\"Bittersweet\", \"Uplifting\", \"Nature-Inspired\", \"Meditation\"]\n",
        "print(\"Choose a mood to generate music:\")\n",
        "for idx, mood in enumerate(moods, 1):\n",
        "    print(f\"{idx} - {mood}\")\n",
        "mood_choice = int(input(\"Enter the number of your choice: \"))\n",
        "config = set_mood_config(mood_choice)\n",
        "mood_name = moods[mood_choice - 1]\n",
        "# Generate and save music for the selected mood\n",
        "generate_sample(generator, latent_dim, i, config, mood_name)"
      ],
      "metadata": {
        "id": "L45WnyM6sbLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}